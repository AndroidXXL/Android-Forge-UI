{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwaJ0eGHCkw"
      },
      "source": [
        "# LoRA Easy Training Colab (Collab-Stand Alone Version)\n",
        "Original Author: Jelosus1\n",
        "\n",
        "\n",
        "Adapter: AndroidXL\n",
        "\n",
        "### Colab powered by [Lora_Easy_Training_Scripts_Backend](https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Learn how to use the colab [here](https://civitai.com/articles/4409).\n",
        "\n",
        "If you feel something is missing, want something to be added or simply found a bug, open an [issue](https://github.com/Jelosus2/Lora_Easy_Training_Colab/issues).\n",
        "\n",
        "---\n",
        "\n",
        "Last Update: November 16, 2024. Check the [full changelog](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#changelog)\n",
        "\n",
        "Changes:\n",
        "- Added emojis to make sections separation easy to the eyes.\n",
        "- Added Illustrious v0.1 and NoobAI 1.0 (Epsilon) to the list of default checkpoints available to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSz_rmldHZvh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ## 1. Install the trainer ![doro](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro.png)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "root_path = Path(\"/content\")\n",
        "trainer_dir = root_path.joinpath(\"trainer\")\n",
        "\n",
        "venv_pip = trainer_dir.joinpath(\"sd_scripts/venv/bin/pip\")\n",
        "venv_python = trainer_dir.joinpath(\"sd_scripts/venv/bin/python\")\n",
        "\n",
        "# @markdown Execute the cell to install the trainer\n",
        "\n",
        "installed_dependencies = False\n",
        "first_step_done = False\n",
        "\n",
        "def install_trainer():\n",
        "  global installed_dependencies, first_step_done\n",
        "\n",
        "  print(\"Installing trainer...\")\n",
        "  !apt -y update -qq\n",
        "  !apt install -y python3.10-venv aria2 -qq\n",
        "\n",
        "  installed_dependencies = True\n",
        "\n",
        "  !git clone https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n",
        "\n",
        "  !chmod 755 /content/trainer/colab_install.sh\n",
        "  os.chdir(trainer_dir)\n",
        "  !./colab_install.sh\n",
        "\n",
        "  os.chdir(root_path)\n",
        "\n",
        "  first_step_done = True\n",
        "  print(\"Installation complete!\")\n",
        "\n",
        "def download_custom_wd_tagger():\n",
        "  global wd_path\n",
        "\n",
        "  wd_path = trainer_dir.joinpath(\"sd_scripts/finetune/tag_images_by_wd14_tagger.py\")\n",
        "\n",
        "  print(\"Downloading tagger script that allows v3 taggers...\")\n",
        "  !rm \"{wd_path}\"\n",
        "  !aria2c \"https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/main/custom/tag_images_by_wd14_tagger.py\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{wd_path}\"\n",
        "\n",
        "def fix_scripts_logging():\n",
        "  print(\"Fixing sd_scripts logging issue on colab...\")\n",
        "  !yes | {venv_pip} uninstall rich\n",
        "\n",
        "def main():\n",
        "  install_trainer()\n",
        "  download_custom_wd_tagger()\n",
        "  fix_scripts_logging()\n",
        "  print(\"Finished installation!\")\n",
        "\n",
        "try:\n",
        "  main()\n",
        "except Exception as e:\n",
        "  print(f\"Error intalling the trainer!\\n{e}\")\n",
        "  first_step_done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oS4dJqXoiyC5"
      },
      "outputs": [],
      "source": [
        "# @title ## 2. Setup the directories ![doro diamond](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_diamond.png)\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "if not globals().get(\"first_step_done\"):\n",
        "  root_path = Path(\"/content\")\n",
        "  trainer_dir = root_path.joinpath(\"trainer\")\n",
        "\n",
        "drive_dir = root_path.joinpath(\"drive/MyDrive\")\n",
        "pretrained_model_dir = root_path.joinpath(\"pretrained_model\")\n",
        "vae_dir = root_path.joinpath(\"vae\")\n",
        "tagger_models_dir = root_path.joinpath(\"tagger_models\")\n",
        "\n",
        "# @markdown The base path for your project. Make sure it can be used as a folder name\n",
        "project_path = \"Loras/Lora_Name\" # @param {type: \"string\"}\n",
        "# @markdown Specify the name for the directories. If you have multiple datasets, separate each with a comma `(,)` like this: **dataset1, dataset2, ...**\n",
        "\n",
        "# @markdown The directory where the results of the training will be stored.\n",
        "output_dir_name = \"output\" # @param {type: \"string\"}\n",
        "# @markdown The directory where your dataset(s) will be located.\n",
        "dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n",
        "# @markdown Use Drive to store all the files and directories\n",
        "use_drive = True # @param {type: \"boolean\"}\n",
        "\n",
        "project_path = project_path.replace(\" \", \"_\")\n",
        "output_dir_name = output_dir_name.replace(\" \", \"_\")\n",
        "\n",
        "second_step_done = False\n",
        "\n",
        "def is_valid_folder_name(folder_name: str) -> bool:\n",
        "  invalid_characters = '<>:\"/\\|?*'\n",
        "\n",
        "  if any(char in invalid_characters for char in folder_name):\n",
        "    return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def mount_drive_dir() -> Path:\n",
        "  base_dir = root_path.joinpath(project_path)\n",
        "\n",
        "  if use_drive:\n",
        "    if not Path(drive_dir).exists():\n",
        "      drive.mount(Path(drive_dir).parent.as_posix())\n",
        "    base_dir = drive_dir.joinpath(project_path)\n",
        "\n",
        "  return base_dir\n",
        "\n",
        "def make_directories():\n",
        "  mount_drive = mount_drive_dir()\n",
        "  output_dir = mount_drive.joinpath(output_dir_name)\n",
        "\n",
        "  if not Path(mount_drive).exists():\n",
        "    Path(mount_drive).mkdir(exist_ok=True)\n",
        "\n",
        "  for dir in [pretrained_model_dir, vae_dir, output_dir, tagger_models_dir]:\n",
        "    Path(dir).mkdir(exist_ok=True)\n",
        "\n",
        "  for dataset_m_dir in dataset_dir_name.replace(\" \", \"\").split(','):\n",
        "    if is_valid_folder_name(dataset_m_dir):\n",
        "      Path(mount_drive.joinpath(dataset_m_dir)).mkdir(exist_ok=True)\n",
        "    else:\n",
        "      print(f\"{dataset_m_dir} is not a valid name for a folder\")\n",
        "      return\n",
        "\n",
        "def main():\n",
        "  for name in [project_path, output_dir_name]:\n",
        "      if not is_valid_folder_name(name.replace(\"/\", \"\") if project_path == name else name):\n",
        "        print(f\"{name} is not a valid name for a folder\")\n",
        "        return\n",
        "\n",
        "  print(\"Setting up directories...\")\n",
        "  make_directories()\n",
        "  print(\"Done!\")\n",
        "\n",
        "try:\n",
        "  main()\n",
        "  second_step_done = True\n",
        "except Exception as e:\n",
        "  print(f\"Error setting up the directories!\\n{e}\")\n",
        "  second_step_done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0_HNDa7Zdei",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ## 3. Download the base model and/or VAE used for training ![doro fubuki](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_fubuki.png)\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "model_url = \"\"\n",
        "vae_url = \"\"\n",
        "\n",
        "# @markdown Default models are provided here for training. If you want to use another one, introduce the URL in the input below. The link must be pointing to either Civitai or Hugging Face and have the correct format. You can check how to get the correct link [here](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae).\n",
        "training_model = \"(XL) Illustrious v0.1\" # @param [\"(XL) PonyDiffusion v6\", \"(XL) NoobAI Epsilon v1.0\", \"(XL) Illustrious v0.1\", \"(XL) Animagine 3.1\", \"(XL) SDXL 1.0\", \"(1.5) anime-full-final-pruned (Most used on Anime LoRAs)\", \"(1.5) AnyLora\", \"(1.5) SD 1.5\"]\n",
        "custom_training_model = \"\" # @param {type: \"string\"}\n",
        "# @markdown The name you want to give to the downloaded model file, if not specified default ones will be used.\n",
        "model_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown VAE used for training. It's not needed for 1.5 nor XL, but it's recommended to use the SDXL base VAE for XL training. If you want to use a custom one, introduce the URL in the input below.\n",
        "vae = \"SDXL VAE\" # @param [\"SDXL VAE\", \"None\"]\n",
        "custom_vae = \"\" # @param {type: \"string\"}\n",
        "# @markdown The name you want to give to the downloaded VAE file, if not specified default ones will be used.\n",
        "vae_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown Introduce your [Civitai API Token](https://civitai.com/user/account) or [HuggingFace Access Token](https://huggingface.co/settings/tokens) if the authentication fails while downloading the model and/or VAE.\n",
        "api_token = \"\" # @param {type: \"string\"}\n",
        "# @markdown You can optionally download the model and/or VAE on your drive so you don't need to download them again in the next session. You only would need to specify their path on the UI for the next time you want to use them.\n",
        "download_in_drive = False # @param {type: \"boolean\"}\n",
        "\n",
        "thrid_step_done = False\n",
        "\n",
        "if custom_training_model:\n",
        "  model_url = custom_training_model\n",
        "elif \"Pony\" in training_model:\n",
        "  model_url = \"https://huggingface.co/AstraliteHeart/pony-diffusion-v6/resolve/main/v6.safetensors\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.1/resolve/main/animagine-xl-3.1.safetensors\"\n",
        "elif \"SDXL\" in training_model:\n",
        "  model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
        "elif \"anime\" in training_model:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
        "elif \"Any\" in training_model:\n",
        "  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.safetensors\"\n",
        "elif \"SD 1.5\" in training_model:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "elif \"Illustrious\" in training_model:\n",
        "  model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0/resolve/main/Illustrious-XL-v0.1.safetensors\"\n",
        "elif \"NoobAI\" in training_model:\n",
        "  model_url = \"https://huggingface.co/Laxhar/noobai-XL-1.0/resolve/main/NoobAI-XL-v1.0.safetensors\"\n",
        "\n",
        "if custom_vae:\n",
        "  vae_url = custom_vae\n",
        "elif \"SDXL\" in vae:\n",
        "  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n",
        "\n",
        "model_file = \"\"\n",
        "vae_file = \"\"\n",
        "\n",
        "header = \"\"\n",
        "\n",
        "if not \"installed_dependencies\" in globals():\n",
        "  print(\"Installing missing dependency...\")\n",
        "  !apt -y update -qq\n",
        "  !apt install -y aria2 -qq\n",
        "  globals().setdefault(\"installed_dependencies\", True)\n",
        "\n",
        "def download_model():\n",
        "  global model_file, model_url, pretrained_model_dir\n",
        "\n",
        "  if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url):\n",
        "    model_url = model_url.replace(\"blob\", \"resolve\")\n",
        "  elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", model_url):\n",
        "    if m := re.search(r\"modelVersionId=(\\d+)\", model_url):\n",
        "      model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "  elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", model_url):\n",
        "    print(\"Invalid model download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n",
        "    return\n",
        "\n",
        "  if \"civitai.com\" in model_url and api_token and not \"hf\" in api_token:\n",
        "    model_url = f\"{model_url}&token={api_token}\" if \"?\" in model_url else f\"{model_url}?token={api_token}\"\n",
        "  elif \"huggingface.co\" in model_url and api_token:\n",
        "    header = f\"Authorization: Bearer {api_token}\"\n",
        "\n",
        "  stripped_model_url = model_url.strip()\n",
        "\n",
        "  if download_in_drive:\n",
        "    pretrained_model_dir = Path(drive_dir).joinpath(\"Downloaded_models\")\n",
        "\n",
        "    if not Path(pretrained_model_dir).exists():\n",
        "      Path(pretrained_model_dir).mkdir(exist_ok=True)\n",
        "\n",
        "  if model_name:\n",
        "    validated_name = model_name.translate(str.maketrans('', '', '\\\\/:*?\"<>|'))\n",
        "\n",
        "    if not validated_name.endswith((\".ckpt\", \".safetensors\")):\n",
        "      model_file = pretrained_model_dir.joinpath(f\"{validated_name}.safetensors\")\n",
        "    else:\n",
        "      model_file = pretrained_model_dir.joinpath(validated_name)\n",
        "  elif stripped_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = pretrained_model_dir.joinpath(stripped_model_url[stripped_model_url.rfind('/'):].replace(\"/\", \"\"))\n",
        "  else:\n",
        "    model_file = pretrained_model_dir.joinpath(\"downloaded_model.safetensors\")\n",
        "    if Path(model_file).exists() and not download_in_drive:\n",
        "      !rm \"{model_file}\"\n",
        "\n",
        "  print(f\"Downloading model from {model_url}...\")\n",
        "  !aria2c \"{model_url}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "def download_vae():\n",
        "  global vae_file, vae_url, vae_dir\n",
        "\n",
        "  if not vae == \"None\":\n",
        "    if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url):\n",
        "      vae_url = vae_url.replace(\"blob\", \"resolve\")\n",
        "    elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", vae_url):\n",
        "      if m := re.search(r\"modelVersionId=(\\d+)\", vae_url):\n",
        "        vae_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "    elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", vae_url):\n",
        "      print(\"Invalid VAE download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n",
        "      return\n",
        "\n",
        "    if \"civitai.com\" in vae_url and api_token and not \"hf\" in api_token:\n",
        "      vae_url = f\"{vae_url}&token={api_token}\" if \"?\" in vae_url else f\"{vae_url}?token={api_token}\"\n",
        "    elif \"huggingface.co\" in vae_url and api_token:\n",
        "      header = f\"Authorization: Bearer {api_token}\"\n",
        "\n",
        "    stripped_model_vae = vae_url.strip()\n",
        "\n",
        "    if download_in_drive:\n",
        "      vae_dir = Path(drive_dir).joinpath(\"Downloaded_VAEs\")\n",
        "\n",
        "      if not Path(vae_dir).exists():\n",
        "        Path(vae_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    if vae_name:\n",
        "      validated_name = vae_name.translate(str.maketrans('', '', '\\\\/:*?\"<>|'))\n",
        "\n",
        "      if not validated_name.endswith((\".ckpt\", \".safetensors\")):\n",
        "        vae_file = vae_dir.joinpath(f\"{validated_name}.safetensors\")\n",
        "      else:\n",
        "        vae_file = vae_dir.joinpath(validated_name)\n",
        "    elif stripped_model_vae.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "      vae_file = vae_dir.joinpath(stripped_model_vae[stripped_model_vae.rfind('/'):].replace(\"/\", \"\"))\n",
        "    else:\n",
        "      vae_file = vae_dir.joinpath(\"downloaded_vae.safetensors\")\n",
        "      if Path(vae_file).exists() and not download_in_drive:\n",
        "        !rm \"{vae_file}\"\n",
        "\n",
        "    print(f\"Downloading vae from {vae_url}...\")\n",
        "    !aria2c \"{vae_url}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n",
        "  else:\n",
        "    vae_file = \"\"\n",
        "\n",
        "def main():\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You have to run the 2nd step first!\")\n",
        "    return\n",
        "\n",
        "  if download_in_drive and not use_drive:\n",
        "    print(\"You are trying to download the model and/or VAE in your drive but you didn't mount it. Please select the 'use_drive' option in 2nd step.\")\n",
        "    return\n",
        "\n",
        "  download_model()\n",
        "  download_vae()\n",
        "\n",
        "try:\n",
        "  main()\n",
        "  thrid_step_done = True\n",
        "except Exception as e:\n",
        "  print(f\"Failed to download the models\\n{e}\")\n",
        "  thrid_step_done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "66XBK6B_iSYj"
      },
      "outputs": [],
      "source": [
        "# @title ## 4. Upload your dataset ![doro shifty](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_shifty.png)\n",
        "import re\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown ### Unzip the dataset\n",
        "# @markdown If you have a dataset in a zip file, you can specify the path to it below. This will extract the dataset into the dataset directory specified in step 2. It supports downloading the zip from **HuggingFace**. To get the correct link you only need to follow the steps [for models/VAEs](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#from-huggingface) but applying them to the zip file.\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/Loras/Datasets/dataset.zip\" # @param {type: \"string\"}\n",
        "# @markdown Specify the name of your dataset directory. If it doesn't exist, it will be created. If you have multiple dataset directories, extract each zip file into its respective dataset directory.\n",
        "extract_to_dataset_dir = \"dataset\" # @param {type: \"string\"}\n",
        "# @markdown Provide a [HuggingFace Access Token](https://huggingface.co/settings/tokens) if your dataset is in a private repository.\n",
        "hf_token = \"\" # @param {type: \"string\"}\n",
        "\n",
        "if not \"installed_dependencies\" in globals():\n",
        "  print(\"Installing missing dependency...\")\n",
        "  !apt -y update -qq\n",
        "  !apt install -y aria2 -qq\n",
        "  globals().setdefault(\"installed_dependencies\", True)\n",
        "\n",
        "def extract_dataset():\n",
        "  global zip_path\n",
        "  is_from_hf = False\n",
        "\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  if zip_path.startswith(\"https://huggingface.co/\"):\n",
        "    is_from_hf = True\n",
        "\n",
        "  if not Path(zip_path).exists() and not is_from_hf:\n",
        "    print(\"The path of the zip doesn't exists!\")\n",
        "    return\n",
        "\n",
        "  if \"drive/MyDrive\" in zip_path and not Path(drive_dir).exists():\n",
        "    print(\"Your trying to access drive but you didn't mount it!\")\n",
        "    return\n",
        "\n",
        "  dataset_dir = root_path.joinpath(project_path, extract_to_dataset_dir)\n",
        "  if Path(drive_dir).exists():\n",
        "    dataset_dir = drive_dir.joinpath(project_path, extract_to_dataset_dir)\n",
        "\n",
        "  if not Path(dataset_dir).exists():\n",
        "    Path(dataset_dir).mkdir(exist_ok=True)\n",
        "    print(f\"Created dataset directory on new location because it didn't exist before: {dataset_dir}\")\n",
        "\n",
        "  if is_from_hf and re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n",
        "    print(\"Zip file from HuggingFace detected, attempting to download...\")\n",
        "\n",
        "    if \"blob\" in zip_path:\n",
        "      zip_path = zip_path.replace(\"blob\", \"resolve\")\n",
        "    header = f\"Authorization: Bearer {hf_token}\" if hf_token else \"\"\n",
        "\n",
        "    !aria2c \"{zip_path}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"/content/dataset.zip\"\n",
        "    zip_path = \"/content/dataset.zip\"\n",
        "  elif is_from_hf and not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n",
        "    print(\"Invalid URL provided for downloading the zip file.\")\n",
        "    return\n",
        "\n",
        "  print(\"Extracting dataset...\")\n",
        "\n",
        "  with zipfile.ZipFile(zip_path, 'r') as f:\n",
        "    f.extractall(dataset_dir)\n",
        "\n",
        "  print(f\"Dataset extracted in {dataset_dir}\")\n",
        "\n",
        "  if is_from_hf:\n",
        "    print(\"Removing temporary zip file...\")\n",
        "    !rm \"{zip_path}\"\n",
        "    print(\"Done!\")\n",
        "\n",
        "extract_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J86M4s3ohUYv",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown ### Tag your images ![doro syuen](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_syuen.png)\n",
        "import os\n",
        "from pathlib import Path\n",
        "!pip install onnxruntime\n",
        "\n",
        "# @markdown As the name suggests, this is the type of tagging you want for your dataset.\n",
        "method = \"Anime\" # @param [\"Anime\", \"Photorealistic\"]\n",
        "# @markdown `(Only applies to Anime method)` The default model used for tagging is `SmilingWolf/wd-eva02-large-tagger-v3`. I find it more accurate than other taggers, but if you have experience, you can use another one and tweak the parameters. If you don't, the default configuration should be fine.\n",
        "model = \"SmilingWolf/wd-eva02-large-tagger-v3\" # @param [\"SmilingWolf/wd-eva02-large-tagger-v3\", \"SmilingWolf/wd-vit-large-tagger-v3\", \"SmilingWolf/wd-swinv2-tagger-v3\", \"SmilingWolf/wd-vit-tagger-v3\", \"SmilingWolf/wd-convnext-tagger-v3\", \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"SmilingWolf/wd-v1-4-moat-tagger-v2\", \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\", \"SmilingWolf/wd-v1-4-convnext-tagger-v2\", \"SmilingWolf/wd-v1-4-vit-tagger-v2\"]\n",
        "# @markdown The directory name of the dataset you want to tag. You can specify another directory when the previous one is fully tagged, in case you have more than one dataset.\n",
        "dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n",
        "# @markdown The type of file to save your captions.\n",
        "file_extension = \".txt\" # @param [\".txt\", \".caption\"]\n",
        "# @markdown `(Only applies to Anime method)` Specify the tags that you don't want the autotagger to use. Separate each one with a comma `(,)` like this: **1girl, solo, standing, ...**\n",
        "blacklisted_tags = \"\" # @param {type: \"string\"}\n",
        "# @markdown `(Only applies to Anime method)` Specify the minimum confidence level required for assigning a tag to the image. A lower threshold results in more tags being assigned. The recommended default value for v2 taggers is 0.35 and for v3 is 0.25.\n",
        "threshold = 0.25 # @param {type: \"slider\", min:0.0, max: 1.0, step:0.01}\n",
        "# @markdown `(Only applies to Photorealistic method)` Specify the minimum number of words (also known as tokens) to include in the captions.\n",
        "caption_min = 10 # @param {type: \"number\"}\n",
        "# @markdown `(Only applies to Photorealistic method)` Specify the maximum number of words (also known as tokens) to include in the captions.\n",
        "caption_max = 75 # @param {type: \"number\"}\n",
        "\n",
        "blacklisted_tags = blacklisted_tags.replace(\" \", \"\")\n",
        "\n",
        "def caption_images():\n",
        "  global use_onnx_runtime\n",
        "\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  dataset_dir = root_path.joinpath(project_path, dataset_dir_name)\n",
        "  if Path(drive_dir).exists():\n",
        "    dataset_dir = drive_dir.joinpath(project_path, dataset_dir_name)\n",
        "\n",
        "  sd_scripts = trainer_dir.joinpath(\"sd_scripts\")\n",
        "  if not globals().get(\"first_step_done\"):\n",
        "    print(\"Please run the step 1 first.\")\n",
        "    return\n",
        "\n",
        "  if True:\n",
        "    print(\"Installing missing dependencies...\")\n",
        "    !{venv_pip} install fairscale==0.4.13 timm==0.6.12\n",
        "    !{venv_pip} install onnxruntime-gpu==1.17.1 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
        "    globals().setdefault(\"tagger_dependencies\", True)\n",
        "\n",
        "  batch_size = 8 if \"v3\" in model or \"swinv2\" in model else 1\n",
        "\n",
        "  model_dir = tagger_models_dir.joinpath(model.split(\"/\")[-1])\n",
        "\n",
        "  print(\"Tagging images\")\n",
        "\n",
        "  if method == \"Anime\":\n",
        "    !{venv_python} {wd_path} \\\n",
        "      {dataset_dir} \\\n",
        "      --repo_id={model} \\\n",
        "      --model_dir={model_dir} \\\n",
        "      --thresh={threshold} \\\n",
        "      --batch_size={batch_size} \\\n",
        "      --max_data_loader_n_workers=2 \\\n",
        "      --caption_extension={file_extension} \\\n",
        "      --undesired_tags={blacklisted_tags} \\\n",
        "      --remove_underscore \\\n",
        "      --onnx\n",
        "  else:\n",
        "    os.chdir(sd_scripts)\n",
        "    !{venv_python} finetune/make_captions.py \\\n",
        "      {dataset_dir} \\\n",
        "      --beam_search \\\n",
        "      --max_data_loader_n_workers=2 \\\n",
        "      --batch_size=8 \\\n",
        "      --min_length={caption_min} \\\n",
        "      --max_length={caption_max} \\\n",
        "      --caption_extension=.txt\n",
        "    os.chdir(root_path)\n",
        "\n",
        "  print(\"Tagging complete!\")\n",
        "\n",
        "caption_images()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Make python code that allows me to append a string to the beginning of all the text files in a certain defined directory, make it so i can define the string and directory in a google collab cell form, write markdown text to show what this cell si for, telling me this cell is used for appending a trigger tag\n",
        "\n",
        "# @title ## Append Trigger Word to Captions\n",
        "# @markdown This cell appends a specified trigger word to the beginning of all text files in a specified directory.\n",
        "\n",
        "trigger_word = \"trigger_word,\"  # @param {type:\"string\"}\n",
        "directory_path = dataset_dir\n",
        "\n",
        "import os\n",
        "\n",
        "def append_trigger(directory, trigger):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):  # Process only .txt files\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            try:\n",
        "                with open(filepath, 'r') as f:\n",
        "                    content = f.read()\n",
        "                with open(filepath, 'w') as f:\n",
        "                    f.write(trigger + \" \" + content)\n",
        "                print(f\"Trigger word appended to: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "append_trigger(directory_path, trigger_word)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nCU6uRT1kFgT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Make a cell which removes a certain string in all text files within a defined directory, where I can define the directory using google collab cell forms, the markdoown text should say remove, and allow me to define it\n",
        "\n",
        "# @title ## Remove String from Text Files\n",
        "# @markdown This cell removes a specified string from all text files in a specified directory.\n",
        "\n",
        "string_to_remove = \", tag_name\"  # @param {type:\"string\"}\n",
        "directory_path = dataset_dir\n",
        "\n",
        "import os\n",
        "\n",
        "def remove_string_from_files(directory, string):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):  # Process only .txt files\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            try:\n",
        "                with open(filepath, 'r') as f:\n",
        "                    content = f.read()\n",
        "                new_content = content.replace(string, \"\") # Remove the string\n",
        "                with open(filepath, 'w') as f:\n",
        "                    f.write(new_content)\n",
        "                print(f\"String removed from: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "remove_string_from_files(directory_path, string_to_remove)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y-gIzBQTmm9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import re\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# @title ## Move files from different dataset directories to one directory\n",
        "# @markdown Enter the paths to the directories you want to copy files from, separated by commas.\n",
        "directories_to_copy = \"directory_path1, directory_path2, ...\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown Specify the name of your dataset directory.\n",
        "dataset_directory = \"final_dataset_directory_path\" # @param {type: \"string\"}\n",
        "\n",
        "# Function to copy files from multiple directories to a destination directory\n",
        "def copy_files_from_multiple_directories(source_dirs, destination_dir):\n",
        "    for source_dir in source_dirs:\n",
        "        if os.path.isdir(source_dir):\n",
        "            dir_name = os.path.basename(source_dir)\n",
        "            print(f\"Copying files from: {source_dir}\")\n",
        "            for filename in os.listdir(source_dir):\n",
        "                source_path = os.path.join(source_dir, filename)\n",
        "                new_filename = f\"{dir_name}_{filename}\"\n",
        "                destination_path = os.path.join(destination_dir, new_filename)\n",
        "                if os.path.isfile(source_path):\n",
        "                    try:\n",
        "                        shutil.copy2(source_path, destination_path) # copy2 preserves metadata\n",
        "                        print(f\"Copied: {new_filename}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error copying {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: Source directory not found: {source_dir}\")\n",
        "\n",
        "# Processing the directories string from the Google Form\n",
        "source_directories = [dir.strip() for dir in directories_to_copy.split(\",\") if dir.strip()]\n",
        "\n",
        "# Create the dataset directory if it doesn't exist\n",
        "dataset_path = Path(dataset_directory)\n",
        "dataset_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Copy files to the dataset directory\n",
        "copy_files_from_multiple_directories(source_directories, str(dataset_path))\n",
        "\n",
        "print(\"Copying complete!\")\n"
      ],
      "metadata": {
        "id": "6nc6WSk8lgfo",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PC5JsouHTr26"
      },
      "outputs": [],
      "source": [
        "# @title ## 5. Start the training ![doro cinderella](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_cinderella.png)\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown Execute this cell to obtain the paths to fill in the paths below.\n",
        "\n",
        "def print_paths():\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  dataset_dirs = []\n",
        "  project_base_dir = root_path.joinpath(project_path)\n",
        "  if globals().get(\"use_drive\"):\n",
        "    project_base_dir = drive_dir.joinpath(project_path)\n",
        "\n",
        "  for id, p_dataset_m_dir in enumerate(dataset_dir_name.replace(\" \", \"\").split(',')):\n",
        "    dataset_dirs.append(f\"Dataset directory {id + 1}: {project_base_dir.joinpath(p_dataset_m_dir)}\")\n",
        "\n",
        "  model_path = model_file or \"None or you didn't run the cell to download it either because you forgot or because you have the model in drive\"\n",
        "  vae_path = vae_file or \"None or you didn't run the cell to download it either because you forgot or because you have the VAE in drive\"\n",
        "  output_path = project_base_dir.joinpath(output_dir_name)\n",
        "\n",
        "  print(\"Dataset paths:\\n  {0}\\nModel path: {1}\\nVAE path: {2}\\nOutput path: {3}\\nConfig file path: {4}\\nTags file path: {4}\".format('\\n  '.join(dataset_dirs), model_path.as_posix().replace(\" \", \"\"), vae_path, output_path, \"It's saved locally on your machine\"))\n",
        "\n",
        "print_paths()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ### Configuration for Dataset and Training\n",
        "\n",
        "# if runtime_store folder doesnt exist, make it\n",
        "os.makedirs(\"/content/trainer/runtime_store\", exist_ok=True)\n",
        "\n",
        "# @markdown #### Dataset Settings\n",
        "resolution = 768 # @param {type: \"number\"}\n",
        "batch_size = 4 # @param {type: \"number\"}\n",
        "enable_bucket = True # @param {type: \"boolean\"}\n",
        "min_bucket_reso = 256 # @param {type: \"number\"}\n",
        "max_bucket_reso = 4096 # @param {type: \"number\"}\n",
        "bucket_reso_steps = 64 # @param {type: \"number\"}\n",
        "caption_extension = \".txt\" # @param [\".txt\", \".caption\"]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/Lora_Name/dataset\" # @param {type: \"string\"}\n",
        "num_repeats = 2 # @param {type: \"number\"}\n",
        "shuffle_caption = True # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown #### Training Settings\n",
        "max_data_loader_n_workers = 1 # @param {type: \"number\"}\n",
        "persistent_data_loader_workers = True # @param {type: \"boolean\"}\n",
        "pretrained_model_name_or_path = \"/content/pretrained_model/Illustrious-XL-v0.1.safetensors\" # @param {type: \"string\"}\n",
        "vae = \"/content/vae/sdxl_vae.safetensors\" # @param {type: \"string\"}\n",
        "no_half_vae = True # @param {type: \"boolean\"}\n",
        "full_bf16 = True # @param {type: \"boolean\"}\n",
        "mixed_precision = \"bf16\" # @param [\"fp16\", \"bf16\"]\n",
        "gradient_checkpointing = True # @param {type: \"boolean\"}\n",
        "seed = 69 # @param {type: \"number\"}\n",
        "max_token_length = 225 # @param {type: \"number\"}\n",
        "prior_loss_weight = 1.0 # @param {type: \"number\"}\n",
        "sdpa = True # @param {type: \"boolean\"}\n",
        "max_train_epochs = 10 # @param {type: \"number\"}\n",
        "cache_latents = True # @param {type: \"boolean\"}\n",
        "network_dim = 8 # @param {type: \"number\"}\n",
        "network_alpha = 4.0 # @param {type: \"number\"}\n",
        "max_timestep = 1000 # @param {type: \"number\"}\n",
        "ip_noise_gamma = 0.05 # @param {type: \"number\"}\n",
        "lr_scheduler = \"cosine\" # @param [\"cosine\", \"linear\", \"constant\"]\n",
        "optimizer_type = \"LoraEasyCustomOptimizer.came.CAME\" # @param {type: \"string\"}\n",
        "lr_scheduler_type = \"LoraEasyCustomOptimizer.RexAnnealingWarmRestarts.RexAnnealingWarmRestarts\" # @param {type: \"string\"}\n",
        "loss_type = \"l2\" # @param [\"l1\", \"l2\", \"smooth_l1\"]\n",
        "learning_rate = 3e-05 # @param {type: \"number\"}\n",
        "unet_lr = 3e-05 # @param {type: \"number\"}\n",
        "text_encoder_lr = 7e-06 # @param {type: \"number\"}\n",
        "max_grad_norm = 1.0 # @param {type: \"number\"}\n",
        "lr_scheduler_args = ['min_lr=7e-06', 'gamma=0.9', 'warmup_steps=4', 'first_cycle_max_steps=70'] # @param {type: \"raw\"}\n",
        "optimizer_args = ['weight_decay=0.1', 'betas=0.9, 0.999, 0.99995'] # @param {type: \"raw\"}\n",
        "output_dir = \"/content/drive/MyDrive/Loras/Lora_Name/output\" # @param {type: \"string\"}\n",
        "output_name = \"Lora_Name\" # @param {type: \"string\"}\n",
        "save_precision = \"bf16\" # @param [\"fp16\", \"bf16\"]\n",
        "save_model_as = \"safetensors\" # @param [\"safetensors\", \"ckpt\"]\n",
        "save_every_n_epochs = 1 # @param {type: \"number\"}\n",
        "save_toml = True # @param {type: \"boolean\"}\n",
        "save_toml_location = \"/content/drive/MyDrive/Loras/Lora_Name/output\" # @param {type: \"string\"}\n",
        "noise_offset = 0.0357 # @param {type: \"number\"}\n",
        "multires_noise_iterations = 5 # @param {type: \"number\"}\n",
        "multires_noise_discount = 0.25 # @param {type: \"number\"}\n",
        "network_module = \"networks.lora\" # @param {type: \"string\"}\n",
        "\n",
        "# Create dataset.toml\n",
        "dataset_toml_content = f\"\"\"\n",
        "[general]\n",
        "resolution = {resolution}\n",
        "batch_size = {batch_size}\n",
        "enable_bucket = {str(enable_bucket).lower()}\n",
        "min_bucket_reso = {min_bucket_reso}\n",
        "max_bucket_reso = {max_bucket_reso}\n",
        "bucket_reso_steps = {bucket_reso_steps}\n",
        "\n",
        "[[datasets]]\n",
        "\n",
        "    [[datasets.subsets]]\n",
        "    caption_extension = \"{caption_extension}\"\n",
        "    image_dir = \"{image_dir}\"\n",
        "    num_repeats = {num_repeats}\n",
        "    shuffle_caption = {str(shuffle_caption).lower()}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/trainer/runtime_store/dataset.toml\", \"w\") as f:\n",
        "    f.write(dataset_toml_content)\n",
        "\n",
        "# Create config.toml\n",
        "config_toml_content = f\"\"\"\n",
        "max_data_loader_n_workers = {max_data_loader_n_workers}\n",
        "persistent_data_loader_workers = {str(persistent_data_loader_workers).lower()}\n",
        "pretrained_model_name_or_path = \"{pretrained_model_name_or_path}\"\n",
        "vae = \"{vae}\"\n",
        "no_half_vae = {str(no_half_vae).lower()}\n",
        "full_bf16 = {str(full_bf16).lower()}\n",
        "mixed_precision = \"{mixed_precision}\"\n",
        "gradient_checkpointing = {str(gradient_checkpointing).lower()}\n",
        "seed = {seed}\n",
        "max_token_length = {max_token_length}\n",
        "prior_loss_weight = {prior_loss_weight}\n",
        "sdpa = {str(sdpa).lower()}\n",
        "max_train_epochs = {max_train_epochs}\n",
        "cache_latents = {str(cache_latents).lower()}\n",
        "network_dim = {network_dim}\n",
        "network_alpha = {network_alpha}\n",
        "max_timestep = {max_timestep}\n",
        "ip_noise_gamma = {ip_noise_gamma}\n",
        "lr_scheduler = \"{lr_scheduler}\"\n",
        "optimizer_type = \"{optimizer_type}\"\n",
        "lr_scheduler_type = \"{lr_scheduler_type}\"\n",
        "loss_type = \"{loss_type}\"\n",
        "learning_rate = {learning_rate}\n",
        "unet_lr = {unet_lr}\n",
        "text_encoder_lr = {text_encoder_lr}\n",
        "max_grad_norm = {max_grad_norm}\n",
        "lr_scheduler_args = {lr_scheduler_args}\n",
        "optimizer_args = {optimizer_args}\n",
        "output_dir = \"{output_dir}\"\n",
        "output_name = \"{output_name}\"\n",
        "save_precision = \"{save_precision}\"\n",
        "save_model_as = \"{save_model_as}\"\n",
        "save_every_n_epochs = {save_every_n_epochs}\n",
        "save_toml = {str(save_toml).lower()}\n",
        "save_toml_location = \"{save_toml_location}\"\n",
        "noise_offset = {noise_offset}\n",
        "multires_noise_iterations = {multires_noise_iterations}\n",
        "multires_noise_discount = {multires_noise_discount}\n",
        "network_module = \"{network_module}\"\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/trainer/runtime_store/config.toml\", \"w\") as f:\n",
        "    f.write(config_toml_content)\n",
        "\n",
        "print(\"dataset.toml and config.toml files have been created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI5A49cuEFnq",
        "outputId": "0ab806f8-1ee8-4258-e471-e4ec9d93e2b0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.toml and config.toml files have been created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ufU4_DUl2Rzv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown Run this cell to start the training\n",
        "\n",
        "# @markdown Are you training on sdxl?\n",
        "sdxl = True # @param {type: \"boolean\"}\n",
        "\n",
        "def start_training(is_sdxl: bool):\n",
        "\n",
        "  os.chdir(trainer_dir)\n",
        "\n",
        "  config = Path(\"runtime_store/config.toml\").resolve()\n",
        "  dataset = Path(\"runtime_store/dataset.toml\").resolve()\n",
        "\n",
        "  if not Path(config).exists() and not Path(dataset).exists():\n",
        "    print(\"The required files were not generated while running the above cell, please check again!\")\n",
        "    return\n",
        "\n",
        "  sd_scripts = Path(\"sd_scripts\").resolve()\n",
        "  training_network = \"sdxl_train_network.py\" if is_sdxl else \"train_network.py\"\n",
        "\n",
        "  !{venv_python} {sd_scripts.joinpath(training_network)} \\\n",
        "    --config_file={config} \\\n",
        "    --dataset_config={dataset}\n",
        "\n",
        "  os.chdir(root_path)\n",
        "\n",
        "start_training(sdxl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Utils ![doro anachiro](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_anachiro.png)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown ### LoRA Resizer ![doro grave](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_grave.png)\n",
        "\n",
        "# @markdown The path pointing to the LoRA file you want to resize.\n",
        "lora = \"\" # @param {type: \"string\"}\n",
        "# @markdown `(Optional)` The path of the directory where the resized LoRA will be saved. If not specified the parent directory of the loaded LoRA will be used.\n",
        "output_dir = \"\" # @param {type: \"string\"}\n",
        "# @markdown `(Optional)` The name for the resized LoRA file. If not specified the name of the loaded LoRA will be used appending **_resized** to it.\n",
        "output_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown The precision for saving the resized LoRA. `fp16` is the usual precision to use. **Don't touch unless you know what you are doing!**\n",
        "save_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"float\"]\n",
        "# @markdown The new dimensions, aka dim, for the LoRA.\n",
        "new_dim = 4 # @param {type: \"number\"}\n",
        "# @markdown `(LoCon-like networks only)` The new conv dimensions, aka conv dim, for the LoRA. Only use on networks that are trained with conv. For example: **LoCon, LyCORIS, LoHa, Lokr, etc**. Keep the value less than 1 to omit it's usage.\n",
        "new_conv_dim = 0 # @param {type: \"number\"}\n",
        "# @markdown Enables/disables the usage of `dynamic_method` and `dynamic_param`. **Don't touch unless you know what you are doing!**\n",
        "use_dynamic = False # @param {type: \"boolean\"}\n",
        "# @markdown Method used to calculate the resize. `sv_fro` is the usual method to use.\n",
        "dynamic_method = \"sv_fro\" # @param [\"sv_fro\", \"sv_ratio\", \"sv_cumulative\"]\n",
        "# @markdown Value used by the `dynamic_method` to calculate the resize.\n",
        "dynamic_param = 0.9700 # @param {type: \"number\"}\n",
        "# @markdown Use the GPU resources to resize the LoRA. If disabled it will use the CPU which is **not recommended!**\n",
        "use_gpu = True # @param {type: \"boolean\"}\n",
        "# @markdown Prints in the console the information about the resizing when the process finishes.\n",
        "verbose_printing = False # @param {type: \"boolean\"}\n",
        "# @markdown `(LoCon-like networks only)` Removes the conv dim layers from the LoRA. Only use on networks that are trained with conv. For example: **LoCon, LyCORIS, LoHa, Lokr, etc. Don't touch unless you know what you are doing!**\n",
        "remove_conv_dims = False # @param {type: \"boolean\"}\n",
        "# @markdown Removes the linear dim layers (which is what is trained usually in a LoRA) from the LoRA. **Don't touch unless you know what you are doing!**\n",
        "remove_linear_dims = False # @param {type: \"boolean\"}\n",
        "\n",
        "def validate() -> tuple[bool, bool]:\n",
        "  global output_dir, output_name\n",
        "\n",
        "  failed = False\n",
        "  use_conv = True\n",
        "  if not globals().get(\"first_step_done\"):\n",
        "    print(\"Please run the 1st step first.\")\n",
        "    failed = True\n",
        "\n",
        "  if not Path(lora).is_file() or Path(lora).suffix not in [\".ckpt\", \".safetensors\"]:\n",
        "    print(\"The path to the LoRA file is invalid.\")\n",
        "    failed = True\n",
        "\n",
        "  if not Path(output_dir).is_dir() or not output_dir:\n",
        "    output_dir = Path(output_dir).parent if output_dir else Path(lora).parent\n",
        "    if not output_dir.is_dir():\n",
        "      print(\"The path to the output folder is invalid, or not a folder\")\n",
        "      failed = True\n",
        "    output_dir = output_dir.as_posix()\n",
        "\n",
        "  if not output_name:\n",
        "    output_name = f\"{Path(lora).name.split('.')[0]}_resized\"\n",
        "  else:\n",
        "    output_name = output_name.split(\".\")[0]\n",
        "\n",
        "  if Path(output_dir).joinpath(f\"{output_name}.safetensors\").exists():\n",
        "    idx = 1\n",
        "    temp_name = output_name\n",
        "    while Path(output_dir).joinpath(f\"{output_name}.safetensors\").exists():\n",
        "      output_name = f\"{temp_name}_{idx}\"\n",
        "      idx += 1\n",
        "\n",
        "    print(f\"Duplicated file in the output directory, file name changed to {output_name}\")\n",
        "\n",
        "  if new_dim < 1:\n",
        "    print(\"The new dim must be 1 or greater\")\n",
        "    failed = True\n",
        "\n",
        "  if new_conv_dim < 1:\n",
        "    print(\"Skipping setting new conv dim, using new dim only\")\n",
        "    use_conv = False\n",
        "\n",
        "  if use_dynamic and dynamic_param <= 0:\n",
        "    print(\"The dynamic param must be greater than 0\")\n",
        "    failed = True\n",
        "\n",
        "  return failed, use_conv\n",
        "\n",
        "def resize_lora(use_conv: bool):\n",
        "  output_file = Path(output_dir).joinpath(f\"{output_name}.safetensors\").resolve()\n",
        "\n",
        "  new_conv_arg = f\"--new_conv_rank={new_conv_dim}\" if use_conv else \"\"\n",
        "  dynamic_method_arg = f\"--dynamic_method={dynamic_method}\" if use_dynamic else \"\"\n",
        "  dynamic_param_arg = \"--dynamic_param={0:.4f}\".format(dynamic_param) if use_dynamic else \"\"\n",
        "\n",
        "  os.chdir(trainer_dir)\n",
        "\n",
        "  !{venv_python} {Path(\"utils/resize_lora.py\").resolve()} \\\n",
        "    --model={lora} \\\n",
        "    --save_precision={save_precision} \\\n",
        "    --new_rank={new_dim} \\\n",
        "    --save_to={output_file} \\\n",
        "    {new_conv_arg} \\\n",
        "    {dynamic_method_arg} \\\n",
        "    {dynamic_param_arg} \\\n",
        "    {\"--verbose\" if verbose_printing else \"\"} \\\n",
        "    {\"--device=cuda\" if use_gpu else \"\"} \\\n",
        "    {\"--del_conv\" if remove_conv_dims else \"\"} \\\n",
        "    {\"--del_linear\" if remove_linear_dims else \"\"} \\\n",
        "\n",
        "  os.chdir(root_path)\n",
        "\n",
        "def main():\n",
        "  failed, use_conv = validate()\n",
        "  if failed:\n",
        "    return\n",
        "\n",
        "  resize_lora(use_conv)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "pEf-buIXyDLg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}